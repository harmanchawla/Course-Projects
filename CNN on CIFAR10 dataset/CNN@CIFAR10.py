# -*- coding: utf-8 -*-
"""HW1-CNN-hsc367-sg5591.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18vxZ802NmQ9Dj6HZELFdOf9vbMIo1S9l

# CNN on CIFAR10 Dataset
"""

import torch
import torchvision
import torchvision.transforms as transforms

import matplotlib.pyplot as plt
import numpy as np

import torch.nn as nn
import torch.nn.functional as F

import torch.optim as optim

"""We are now ready to extract, transform and load our data. 

---


For this PyTorch gives us two essential classes: DataLoader and Dataset (abstract classes). DataLoader wraps the data and provides access to the underlying data.
"""

# Defining the transformation parameter which should be performed on the data set. 
transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

# ETL training data
trainset = torchvision.datasets.CIFAR10(
    root='./data', 
    train=True,                          
    download=True, 
    transform=transform)

trainloader = torch.utils.data.DataLoader(
    trainset, 
    batch_size=50,                            
    shuffle=True, 
    num_workers=2)

#ETL test data
testset = torchvision.datasets.CIFAR10(
    root='./data', 
    train=False,                     
    download=True, 
    transform=transform)

testloader = torch.utils.data.DataLoader(
    testset, 
    batch_size=50,                   
    shuffle=False, 
    num_workers=2)

# Checking that everything is in order
print('Length of train set:', len(trainset))
print('Length of test set:', len(testset))

# Defining the classes in CIFAR10 data set. 
# The order is really important here! Please don't change that while checking our code. 

classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

"""Since class imbalance does not occur for our data set, we don't need to account for that in any manner. 

---

**Accessing data samples in code:** To access an indiviual element from the trainloader object, we first pass it to python's built-in **iter() function**. This returns an object representing a stream of data that we can iterate over. 

Python's built-in **next() function** then allows us to get the next data element in the stream. Since we defined our batch-size to be 4 while loading the data, this will consist of 5 elements in each iteration.
"""

# get some random training images
dataiter = iter(trainloader)

#sequence unpacking as dataiter will be a tuple by definition
images, labels = dataiter.next()

"""The **nn.Module** has a forward code method to represent it's forward pass and so when we are building layers and networks, we must provide an implementation of forward() method. The forward method in the nn.Linear (nn.Module subclass), the method returns F.Linear()


```
def forward(self, input):
  return F.linear(input, self.weights, self.bias)
```

So what we need to do is: 
1.   Extend the nn.Module base class
2.   Define layers as class attributes 
2.   Implement the forward() method
 

We are building a CNN here so the types of layers that we will define are: Convolution layers and Linear layers.
"""

# Step 1a: Extending the nn.Module base class for your CNN
class CNN(nn.Module):
    def __init__(self):
        # Step 1b: Call to the super-class constructor
        super(CNN, self).__init__()
        
        # Step2:. Defining layers of the network as attribute
        # A convolution layer will take 3 basic parameters, in-channels, out-channels and kernel size.
        # For our linear layers we will have in-features and out-features. 
        # Using MaxPool for reduction operations

        self.conv1 = nn.Conv2d(3, 6, 5, 1, 2)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5, 1, 2)
        self.conv3 = nn.Conv2d(16, 32, 5, 1, 2)
        self.conv4 = nn.Conv2d(32, 64, 5)
        self.fc1 = nn.Linear(64 * 6 * 6, 60)
        self.fc2 = nn.Linear(60, 20)
        self.out = nn.Linear(20, 10)
        
        
    # Step 3: Implement the forward() method
    def forward(self, x):
      
        # Using ReLU for non-linearity (working on real-world data)
        x = F.relu(self.conv1(x)) # 32*32
        x = self.pool(F.relu(self.conv2(x))) # 16*16
        x = F.relu(self.conv3(x)) # 16*16
        x = self.pool(F.relu(self.conv4(x))) # 6*6
        
        # flatten the tensor
        x = x.view(-1, 64 * 6 * 6)
        x = F.relu(self.fc1(x))
        
        # using ReLU as the activation function
        x = F.dropout(x, p=0.3, training=self.training)
        x = F.relu(self.fc2(x))
        x = self.out(x)
        return x


net = CNN()

# Verifying the network architecture.
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
net.to(device)

for name, param in net.named_parameters():
  print(name, '\t\t', param.shape)

"""**Defining the loss function**
 We are using **log loss** as our loss function. In PyTorch, log loss is given as CrossEntropyLoss(). More about the loss function and it's workings can be found at https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html
 
 During the tutorial in class, we used nll_loss as our loss function. We found an article on PyTorch discussion forums ([click here](https://discuss.pytorch.org/t/is-log-softmax-nllloss-crossentropyloss/9352)) which describes how **log_softmax() + nll_loss() is essentially log_loss()**
"""

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)

net.train()
for epoch in range(100):  # loop over the dataset multiple times

    training_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        # get the inputs
        inputs, labels = data
        inputs, labels = inputs.to(device), labels.to(device)

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward, backward and optimize
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # loss calculated during training
        training_loss += loss.item()
        if i % 1000 == 999:    # print every 4000 mini-batches
            print('[%d, %5d] loss: %f' %(epoch + 1, i + 1, training_loss / 1000))
            training_loss = 0.0

print('Finished Training')

# Getting a stream of data we can iterate over. 
dataiter = iter(testloader)
images, labels = dataiter.next()

images, labels = images.to(device), labels.to(device)

# With the test images now in 'images', we call the CNN we defined earlier
outputs.to(device)
outputs = net(images)

net.eval()
correct = 0
total = 0
prediction = []
with torch.no_grad():
    for data in testloader:
        images, labels = data
        images, labels = images.to(device), labels.to(device)
        outputs = net(images)
        _, predicted = torch.max(outputs.data, 1)
        prediction.append(predicted)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print('Accuracy on test images:', (100 * correct / total))
print(prediction)

"""# Model Architecture & Optimizing Hyperparamters
<hr>
During this assignment, I relied heavily on the PyTorch documentation as well as the tutorial covered in class. I have been describing our programming choices, the activation functions used and the loss function chosen along the way as I thought that would make it easier for you to understand the thought behind the implementation. In this section, I will attempt to lay out the a series of choices that led to the architecture I have implemented. 

The task at hand was to juggle the values of the following parameters: learning rate, epochs, Batch size, activation function, hidden layers and units and dropout for regularization. 

The model we ended up with has 4 convolution layers and a fully-connected model with 2 hidden layers. We use padding during convolution operations to preserve the size of the image and thus the border details. The final convolution doesn't use padding (using it didn't yeild to a better result and increased the number of inputs to the FC) Since we are dealing with real-world data which is rarely linear in nature, we perform the ReLU operation after each convolution layer. We perform a pooling operation (MaxPooling) after the 2nd and the 4th convolution layer as a reduction operation.

We started by simply defining a **2 layer CNN model as the minimum requirement** for the question. We had set the **learning rate at 0.01 as in the online tutorial for MNIST**. Some visualization of the weights and activations revealed a lot of **noise**. This indicated one of the following: 

1.   Network not trained for long enough
2.   Low regularization or Overfitting

We increased the number of convolution layers, lowered the batch size and increase the epochs. This still didn't drastically change our accuracy which was still in the high 50s. Turning our eyes to the activations revealed that the probable cause might be a high learning rate. (dead filters) 

We decreased the learning rate to 0.005 and saw and improvement, but the loss toggled between values. Reducing it further to an even 0.001 seemed to solve this problem to some extent so we kept it that way. 

The dropout rate was set to 0.25 instead of it's default value of 0.5 due to the recommendation by a user on StackOverflow [click here](https://stackoverflow.com/questions/47892505/dropout-rate-guidance-for-hidden-layers-in-a-convolution-neural-network). He linked the following paper which suggested that for a small CNN, a dropout p=0.1-0.3 is much better to prevent overfitting ([paper](http://mipal.snu.ac.kr/images/1/16/Dropout_ACCV2016.pdf)).

Plotting an accuracy vs epochs curve revealed that a higher epochs weren't doing us good as it took longer to train the network (even on the GPU) and the accuracy payoff was about 3-4%. So, we decided to keep the epochs low. This also allowed us to fine-tune other parameters quicker and try a variety of values of the kernels in the CNN layers and number of nodes in the hidden layers. 

The next task was to find a better fit for the 3 CNN (3x3x6 and 3x3x12 and 3x3x16) layer we had begun with. I found this helpful article on stackexchange (sorry, I didn't save the link) which described a general pattern most CNN architectures followed. This involved starting with a small kernel and creating a 'shallow' layer using this kernel. The size of the kernel was to be increased in the subsequent layers with the feature map getting deeper and deeper. The logic (rather intiution) behind such an approach was to allow the initial set of kernels to discover highly localized featues first before moving on to regions/segments - similar to zooming out a camera to see reveal the entire image. 

We didn't use this concept on puporse. After a bit of hit and try, we found that kernels of the same size, be it 3 or 5 (not higher) performed better than the iterative increase method described earlier. It took about 50 epochs before the loss became acceptable. So, if you implement 6 layers of 3x3 kernels each one incresing in depth or a 4 layer 5x5, the results were better. We initialized our first CNN with kernel size 5. Subsequent kernels have a size of (5,5) as using a larger kernel would have resulted in a huge part of the kernel not fitting on the image. For example, a 7x7 kernel on a 16x16 image would have resulted in a 8x8 image. Reducing the size of the input parameter was also essential. Pooling is done twice, each time after 2 convolution operations to lower the size. The architecture of the fully connected layers was borrowed from a previous project I did as a part of my computer vision course and knew that it worked well. 

I kept the batch size low ( = 5) as I was debugging the model on my personal machine. I understand the logic behind larger batch sizes (.e. the gradients are only a very rough approximation of the true gradients, so it’ll take a lot longer to find a good solution) In the final run, I have changed the batch size to 50. 



<hr>
Note: We did find this CNN implementation using CIFAR-10 data on GitHub using TensorFlow ([link](http://parneetk.github.io/blog/cnn-cifar10/)) which used 6 convolution layers (all of kernel size 3) and FC of 2 hidden layer which achieved an accuracy of 80% but took an hour to train.
"""